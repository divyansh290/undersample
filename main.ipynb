{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er494WlJW74J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4kJXRNkwYw_",
    "outputId": "2201d46e-653e-4f36-d1fb-804313779d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores (threads): 2\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(f\"Number of available CPU cores (threads): {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWL2WFMCtMhh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/train.csv'  # Replace with your actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a directory to save the images\n",
    "image_dir = 'downloaded_images'\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Function to download an image from a URL\n",
    "def download_image(row):\n",
    "    image_url = row['image_link']\n",
    "    image_name = f\"{row['group_id']}_{row.name}.jpg\"  # Unique image name using group_id and index\n",
    "    save_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "        return image_url, True  # Return URL and success status\n",
    "    except Exception as e:\n",
    "        return image_url, False  # Return URL and failure status\n",
    "\n",
    "# Number of threads to use\n",
    "num_threads = 80\n",
    "\n",
    "# Use ThreadPoolExecutor to download images in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(download_image, row) for idx, row in data.iterrows()]\n",
    "\n",
    "    # Use tqdm to display progress\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        url, success = future.result()\n",
    "        if not success:\n",
    "            print(f\"Failed to download {url}\")\n",
    "\n",
    "print(\"Image download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHr2MIQDXHo8"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "csv_file = \"/content/maximum_weight_recommendation_data.csv\"  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoTr0CRnqVdC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import joblib\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEh4884GXIi9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import pytesseract\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = ['image_link', 'entity_name', 'entity_value']\n",
    "if not all(col in data.columns for col in required_columns):\n",
    "    raise ValueError(f\"CSV must contain columns: {', '.join(required_columns)}\")\n",
    "\n",
    "# Directory to save images\n",
    "image_dir = \"images\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Configure retry strategy globally (outside the function)\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "    backoff_factor=1\n",
    ")\n",
    "\n",
    "# Adjust pool size and max connections\n",
    "adapter = HTTPAdapter(\n",
    "    max_retries=retry_strategy,\n",
    "    pool_connections=100,  # Increase pool connections limit\n",
    "    pool_maxsize=100       # Increase the max number of connections in the pool\n",
    ")\n",
    "\n",
    "# Setup the session with the adjusted adapter\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "http.mount(\"http://\", adapter)\n",
    "\n",
    "# Pre-compile the regex pattern for efficiency\n",
    "pattern = re.compile(r\"(\\d+\\.?\\d*)\\s*([a-zA-Z]+)\")\n",
    "\n",
    "def process_image(row):\n",
    "    try:\n",
    "        image_url = row['image_link']\n",
    "        entity_name = row['entity_name']\n",
    "        entity_value = row['entity_value']\n",
    "\n",
    "        image_name = f\"image_{row.name}.jpg\"\n",
    "        save_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "        # Download the image\n",
    "        response = http.get(image_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            img.save(save_path)\n",
    "\n",
    "            # Perform OCR on the image\n",
    "            text = pytesseract.image_to_string(img)\n",
    "\n",
    "            # Parse text to get the numerical value and unit\n",
    "            matches = pattern.findall(text)\n",
    "            parsed_text = \" \".join([\" \".join(match) for match in matches]) if matches else \"\"\n",
    "\n",
    "            return {\n",
    "                'image_path': save_path,\n",
    "                'entity_name': entity_name,\n",
    "                'entity_value': entity_value,\n",
    "                'extracted_text': text,\n",
    "                'parsed_text': parsed_text\n",
    "            }\n",
    "        else:\n",
    "            # Log failed download attempts\n",
    "            return {'error': f\"Failed to download {image_url}. Status code: {response.status_code}\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log exceptions instead of printing them\n",
    "        return {'error': f\"Error processing row {row.name}: {str(e)}\"}\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "ocr_results = []\n",
    "with ThreadPoolExecutor(max_workers=min(20, os.cpu_count() * 2)) as executor:\n",
    "    futures = {executor.submit(process_image, row): row.name for _, row in data.iterrows()}\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Images\"):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            ocr_results.append(result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "ocr_df = pd.DataFrame(ocr_results)\n",
    "\n",
    "# Filter out errors from the results if any\n",
    "ocr_df_filtered = ocr_df[ocr_df['error'].isna()]\n",
    "\n",
    "# Save OCR results to CSV for later use\n",
    "ocr_df_filtered.to_csv('ocr_results.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(ocr_df_filtered)} images out of {len(data)} total images successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og5l4RXdqeoW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zn5OiEwA2s1K"
   },
   "outputs": [],
   "source": [
    "ocr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFKl_0lB5RzA"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "_df_0.groupby('image_path').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6ZFHctDt5zK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "ocr_df = pd.read_csv('ocr_results.csv')\n",
    "\n",
    "# Step 2: Check for NaN values and drop them\n",
    "ocr_df = ocr_df.dropna(subset=['parsed_text', 'entity_value'])\n",
    "\n",
    "# Encode your labels\n",
    "ocr_df['entity_value_encoded'] = ocr_df['entity_value'].astype('category').cat.codes\n",
    "\n",
    "# Prepare the dataset\n",
    "texts = ocr_df['parsed_text'].tolist()\n",
    "labels = ocr_df['entity_value_encoded'].tolist()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class EntityDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Step 3: Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create DataLoader\n",
    "max_len = 128  # Maximum length of input sequences\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = EntityDataset(X_train, y_train, tokenizer, max_len)\n",
    "test_dataset = EntityDataset(X_test, y_test, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 4: Load the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(ocr_df['entity_value_encoded'].unique()))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(5):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} completed. Loss: {loss.item()}')\n",
    "\n",
    "# Step 5: Testing the model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LP-m1j6Mqvpv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with a single space\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)  # Remove non-printable characters\n",
    "    return text.strip()\n",
    "\n",
    "# Load the OCR results CSV file\n",
    "ocr_results_path = 'path_to_your_ocr_results.csv'\n",
    "ocr_results_df = pd.read_csv(ocr_results_path)\n",
    "\n",
    "# Clean the extracted_text and parsed_text columns\n",
    "ocr_results_df['clean_extracted_text'] = ocr_results_df['extracted_text'].apply(clean_text)\n",
    "ocr_results_df['clean_parsed_text'] = ocr_results_df['parsed_text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(ocr_results_df[['image_path', 'entity_name', 'entity_value', 'clean_extracted_text', 'clean_parsed_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZfqXcML9BTz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].numpy())\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NhnJcxx4n4p"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with a single space\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)  # Remove non-printable characters\n",
    "    return text.strip()\n",
    "\n",
    "# Load the OCR results CSV file\n",
    "ocr_results_path = '/content/ocr_results.csv'\n",
    "ocr_results_df = pd.read_csv(ocr_results_path)\n",
    "\n",
    "# Clean the extracted_text and parsed_text columns\n",
    "ocr_results_df['clean_extracted_text'] = ocr_results_df['extracted_text'].apply(clean_text)\n",
    "ocr_results_df['clean_parsed_text'] = ocr_results_df['parsed_text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(ocr_results_df[['image_path', 'entity_name', 'entity_value', 'clean_extracted_text', 'clean_parsed_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSFBjVZo42im"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Initialize a blank spaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Function to create training data in the format required by spaCy\n",
    "def create_training_data(df):\n",
    "    training_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['clean_extracted_text']\n",
    "        entities = []\n",
    "        if pd.notna(row['entity_value']):\n",
    "            entity_value = str(row['entity_value'])\n",
    "            start_index = text.find(entity_value)\n",
    "            if start_index != -1:\n",
    "                end_index = start_index + len(entity_value)\n",
    "                entities.append((start_index, end_index, row['entity_name']))\n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "    return training_data\n",
    "\n",
    "training_data = create_training_data(ocr_results_df)\n",
    "\n",
    "# Convert the training data to spaCy's DocBin format\n",
    "doc_bin = DocBin()\n",
    "for text, annotations in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annotations.get('entities'):\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span is not None:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "# Save the training data to a file\n",
    "doc_bin.to_disk(\"training_data.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNk8DMV8B2N_"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Load the training data\n",
    "doc_bin = DocBin().from_disk(\"training_data.spacy\")\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "# Add the NER component to the pipeline if it's not already present\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add the labels to the NER component\n",
    "for _, annotations in training_data:\n",
    "    for start, end, label in annotations.get('entities'):\n",
    "        ner.add_label(label)\n",
    "\n",
    "# Convert the training data to Example objects\n",
    "examples = []\n",
    "for doc, (text, annotations) in zip(docs, training_data):\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    examples.append(example)\n",
    "\n",
    "# Disable other pipes during training to train only NER\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(100):  # Number of training iterations\n",
    "        random.shuffle(examples)\n",
    "        losses = {}\n",
    "        batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            nlp.update(batch, drop=0.5, losses=losses)\n",
    "        print(\"Losses\", losses)\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy00VzzEB6Rf"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the training data\n",
    "doc_bin = DocBin().from_disk(\"training_data.spacy\")\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "# Add the NER component to the pipeline if it's not already present\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add the labels to the NER component\n",
    "for _, annotations in training_data:\n",
    "    for start, end, label in annotations.get('entities'):\n",
    "        ner.add_label(label)\n",
    "\n",
    "# Convert the training data to Example objects\n",
    "examples = []\n",
    "for doc, (text, annotations) in zip(docs, training_data):\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    examples.append(example)\n",
    "\n",
    "# Split into training and validation sets\n",
    "split = int(len(examples) * 0.8)\n",
    "train_examples = examples[:split]\n",
    "val_examples = examples[split:]\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, examples):\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for example in examples:\n",
    "        pred = model(example.text)\n",
    "        preds.extend([(ent.text, ent.label_) for ent in pred.ents])\n",
    "        trues.extend([(ent.text, ent.label_) for ent in example.reference.ents])\n",
    "    return classification_report(trues, preds, output_dict=True)\n",
    "\n",
    "# Disable other pipes during training to train only NER\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(100):  # Number of training iterations\n",
    "        random.shuffle(train_examples)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            nlp.update(batch, drop=0.5, losses=losses)\n",
    "        print(f\"Iteration {itn+1}: Losses\", losses)\n",
    "\n",
    "        # Evaluate the model every 10 iterations\n",
    "        if (itn + 1) % 10 == 0:\n",
    "            metrics = evaluate_model(nlp, val_examples)\n",
    "            print(f\"Iteration {itn+1}: Evaluation Metrics\")\n",
    "            print(metrics)\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EbL1ftlCf77"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the trained model\n",
    "nlp = spacy.load(\"ner_model\")\n",
    "\n",
    "# Evaluate the final model on the validation set\n",
    "metrics = evaluate_model(nlp, val_examples)\n",
    "print(\"Final Evaluation Metrics\")\n",
    "print(metrics)\n",
    "\n",
    "# Detailed classification report\n",
    "report = classification_report([ent[1] for ent in metrics], [ent[1] for ent in metrics], output_dict=False)\n",
    "print(report)\n",
    "\n",
    "# Test the model on new text\n",
    "test_text = \"The product weight is 12.0 ounce.\"\n",
    "doc = nlp(test_text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8y64IZgxCwYQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
